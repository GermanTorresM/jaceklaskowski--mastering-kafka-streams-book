== Kafka Streams -- Stream Processing Library on Apache Kafka

*Kafka Streams* is a library for developing applications for processing records from topics in Apache Kafka.

Kafka Streams applications process records through *topologies*. A Kafka Streams developer defines the processing logic using either the lower-level link:kafka-streams-Topology.adoc[Topology] API directly (and defines a DAG topology of link:kafka-streams-Processor.adoc[Processors]) or indirectly through link:kafka-streams-StreamsBuilder.adoc[StreamsBuilder] that provides the high-level DSL to define transformations.

Once you have defined the processing logic in the form of a Kafka Stream topology, you should define the execution environment of the application using link:kafka-streams-KafkaStreams.adoc[KafkaStreams] API that you link:kafka-streams-KafkaStreams.adoc#start[start] in the end.

[source, scala]
----
object KafkaStreamsApp extends App {

  // Step 1. Describe Topology
  // Consume records from input topic and produce records to upper topic
  import org.apache.kafka.streams.StreamsBuilder
  val builder = new StreamsBuilder

  import org.apache.kafka.streams.Consumed
  import org.apache.kafka.common.serialization.Serdes
  val consumed = Consumed.`with`(Serdes.String, Serdes.String)

  // Read (consume) records from input topic with keys and values being Strings
  import org.apache.kafka.streams.kstream.KStream
  val input: KStream[String, String] = builder.stream("input", consumed)

  // Transform the values of the records to their upper case equivalents
  val upper: KStream[String, String] = input.mapValues { record: String => record.toUpperCase }

  import org.apache.kafka.streams.kstream.Produced
  val produced = Produced.`with`(Serdes.String, Serdes.String)

  // Write (produce) the records out to upper topic
  upper.to("upper", produced)

  // Print out records to stdout for debugging purposes
  import org.apache.kafka.streams.kstream.Printed
  val sysout = Printed
    .toSysOut[String, String]
    .withLabel("stdout")
  upper.print(sysout)

  // Step 2. Build Topology
  import org.apache.kafka.streams.Topology
  val topology: Topology = builder.build

  // You can describe the topology and just finish
  println(topology.describe())

  // That finishes the "declaration" part of developing a Kafka Stream application
  // Nothing is executed at this time (no threads have started yet)

  // Step 3. Specify Configuration
  import java.util.Properties
  val props = new Properties()
  import org.apache.kafka.streams.StreamsConfig
  props.put(StreamsConfig.APPLICATION_ID_CONFIG, "KafkaStreamsApp")
  props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092")
  import org.apache.kafka.streams.StreamsConfig
  val config = new StreamsConfig(props)

  // Step 4. Create Kafka Streams Client
  import org.apache.kafka.streams.KafkaStreams
  val ks = new KafkaStreams(topology, config)

  // Step 5. Start Stream Processing, i.e. Consuming, Processing and Producing Records
  ks.start
}
----

On the outside, a typical Kafka Streams application is made up of two main objects, i.e. <<kafka-streams-Topology.adoc#, Topology>> and <<kafka-streams-KafkaStreams.adoc#, KafkaStreams>> (aka _streams client_).

On the inside, the `KafkaStreams` object creates a <<kafka-streams-StateDirectory.adoc#, StateDirectory>>, a `Metric` (with a `JmxReporter`), a <<kafka-streams-StreamsMetadataState.adoc#, StreamsMetadataState>>, a <<kafka-streams-GlobalStreamThread.adoc#, GlobalStreamThread>> and one or more <<kafka-streams-StreamThread.adoc#, StreamThreads>>.

`KafkaStreams` uses <<kafka-streams-properties.adoc#num.stream.threads, num.stream.threads>> configuration property for the number of `StreamThreads` to create (default: `1` processing thread).

A single `StreamThread` creates a restore Kafka Consumer (`restoreConsumer`), a <<kafka-streams-StoreChangelogReader.adoc#, StoreChangelogReader>>, a <<kafka-streams-StreamsMetricsThreadImpl.adoc#, StreamsMetricsThreadImpl>>, a <<kafka-streams-ThreadCache.adoc#, ThreadCache>>, a <<kafka-streams-TaskCreator.adoc#, TaskCreator>> (for <<kafka-streams-StreamTask.adoc#, StreamTasks>>), a <<kafka-streams-StandbyTaskCreator.adoc#, StandbyTaskCreator>> (for <<kafka-streams-StandbyTask.adoc#, StandbyTasks>>) and the <<kafka-streams-TaskManager.adoc#, TaskManager>>.

It is through the <<kafka-streams-TaskManager.adoc#, TaskManager>> that a `StreamThread` manages the <<kafka-streams-TaskManager.adoc#active, active stream tasks>> and <<kafka-streams-TaskManager.adoc#process, processes records>>.

A `StreamThread` creates also a <<kafka-streams-StreamThread-RebalanceListener.adoc#, RebalanceListener>> for...FIXME

Eventually, `KafkaStreams` is <<kafka-streams-KafkaStreams.adoc#start, started>> and the Kafka Streams application starts consuming, processing, and producing records (as described by the <<kafka-streams-Topology.adoc#, Topology>>).

When <<kafka-streams-KafkaStreams.adoc#start, started>>, `KafkaStreams` starts the <<kafka-streams-GlobalStreamThread.adoc#, GlobalStreamThread>> and the <<kafka-streams-StreamThread.adoc#, StreamThreads>>.

A `StreamThread` <<kafka-streams-StreamThread.adoc#runLoop, runs the main record processing loop>> that uses a https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[KafkaConsumer] to link:++https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe-java.util.Collection-org.apache.kafka.clients.consumer.ConsumerRebalanceListener-++[subscribe] to the <<kafka-streams-InternalTopologyBuilder.adoc#sourceTopicPattern, source topics>> of the topology (with the http://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.html[ConsumerRebalanceListener] for dynamically assigned partitions).

A `StreamThread` then uses the https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[KafkaConsumer] to link:++https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#poll-java.time.Duration-++[fetch records] (from the <<kafka-streams-InternalTopologyBuilder.adoc#sourceTopicPattern, source topics>>).

`StreamThread` uses <<kafka-streams-properties.adoc#poll.ms, poll.ms>> configuration property for the poll time (default: `100L` milliseconds) or `0` when in `PARTITIONS_ASSIGNED` state.

If there are records to process and there are <<kafka-streams-TaskManager.adoc#hasActiveRunningTasks, active streams tasks>>, `StreamThread`...FIXME
