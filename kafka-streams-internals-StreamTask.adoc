== [[StreamTask]] StreamTask

`StreamTask` is a concrete link:kafka-streams-internals-AbstractTask.adoc[stream processor task] that manages a <<partitions, collection of Kafka TopicPartitions>> as a <<partitionGroup, PartitionGroup>>.

`StreamTask` is <<creating-instance, created>> exclusively when `TaskCreator` is requested to <<kafka-streams-internals-TaskCreator.adoc#createTask, create one>>.

When requested to <<initializeTopology, initialize a processor topology>> (as a <<kafka-streams-internals-Task.adoc#, task>>), `StreamTask`...FIXME

[[commitNeeded]]
[[commitRequested]]
[[needCommit]]
`StreamTask` may need a commit (after `ProcessorContextImpl` is requested to <<kafka-streams-internals-ProcessorContextImpl.adoc#commit, commit>>).

[[commitOffsetNeeded]]
`StreamTask` uses `commitOffsetNeeded` flag to...FIXME

[[maxBufferedSize]]
`StreamTask` uses link:kafka-streams-StreamsConfig.adoc#buffered.records.per.partition[buffered.records.per.partition] configuration property to control when to resume a partition (when <<process, processing a single record>>).

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.kafka.streams.processor.internals.StreamTask` logger to see what happens inside.

Add the following line to `log4j.properties`:

```
log4j.logger.org.apache.kafka.streams.processor.internals.StreamTask=ALL
```

Refer to <<kafka-logging.adoc#log4j.properties, Application Logging Using log4j>>.
====

=== StreamTask and RecordCollector

`StreamTask` creates a new <<recordCollector, RecordCollector>> or is given one (when <<creating-instance, created>>).

The <<recordCollector, RecordCollector>> is requested to <<kafka-streams-internals-RecordCollector.adoc#init, initialize>> (with a <<producer, Kafka Producer>>) when `StreamTask` is <<creating-instance, created>> and <<resume, resumed>>.

`StreamTask` uses the <<recordCollector, RecordCollector>> for the following:

* Creating an <<kafka-streams-internals-AbstractTask.adoc#processorContext, InternalProcessorContext>> (when <<creating-instance, created>>)

* <<kafka-streams-internals-RecordCollector.adoc#offsets, Getting offsets>> when <<activeTaskCheckpointableOffsets, activeTaskCheckpointableOffsets>>

* <<kafka-streams-internals-RecordCollector.adoc#flush, Flushing the internal Kafka producer>> when <<flushState, flushState>>

* <<suspend, suspend>> and <<maybeAbortTransactionAndCloseRecordCollector, maybeAbortTransactionAndCloseRecordCollector>>

The <<recordCollector, RecordCollector>> is requested to <<kafka-streams-internals-RecordCollector.adoc#close, close>> when `StreamTask` is requested to <<suspend, suspend>> and <<maybeAbortTransactionAndCloseRecordCollector, maybeAbortTransactionAndCloseRecordCollector>>.

=== [[closeTopology]] `closeTopology` Internal Method

[source, java]
----
void closeTopology()
----

`closeTopology`...FIXME

NOTE: `closeTopology` is used exclusively when `StreamTask` is requested to <<suspend, suspend>>.

=== [[suspend]] `suspend` Method

[source, java]
----
void suspend()  // <1>

// PRIVATE API
suspend(boolean clean, boolean isZombie)
----
<1> Uses the private `suspend` with `clean` enabled and `isZombie` disabled

NOTE: `suspend` is part of <<kafka-streams-internals-Task.adoc#suspend, Task Contract>> to...FIXME

`suspend`...FIXME

NOTE: The private `suspend` is used when `StreamTask` is requested to <<close, close>>.

=== [[close]] `close` Method

[source, java]
----
void close(
  final boolean clean,
  final boolean isZombie)
----

NOTE: `close` is part of link:kafka-streams-internals-Task.adoc#close[Task Contract] to...FIXME.

`close`...FIXME

=== [[creating-instance]] Creating StreamTask Instance

`StreamTask` takes the following to be created:

* [[id]] <<kafka-streams-TaskId.adoc#, TaskId>>
* [[partitions]] Topic partitions (Kafka `TopicPartition`)
* [[topology]] <<kafka-streams-internals-ProcessorTopology.adoc#, ProcessorTopology>>
* [[consumer]] Kafka https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[Consumer] (`Consumer<byte[], byte[]>`)
* [[changelogReader]] <<kafka-streams-ChangelogReader.adoc#, ChangelogReader>>
* [[config]] <<kafka-streams-StreamsConfig.adoc#, StreamsConfig>>
* [[metrics]] <<kafka-streams-internals-StreamsMetricsImpl.adoc#, StreamsMetricsImpl>>
* [[stateDirectory]] <<kafka-streams-internals-StateDirectory.adoc#, StateDirectory>>
* [[cache]] <<kafka-streams-internals-ThreadCache.adoc#, ThreadCache>>
* [[time]] `Time`
* [[producerSupplier]] <<kafka-streams-internals-ProducerSupplier.adoc#, ProducerSupplier>>
* [[recordCollector]] <<kafka-streams-internals-RecordCollector.adoc#, RecordCollector>>
* [[closeSensor]] `closeSensor` Kafka `Sensor`

`StreamTask` initializes the <<internal-properties, internal properties>>.

=== [[initTopology]] `initTopology` Internal Method

[source, java]
----
void initTopology()
----

`initTopology`...FIXME

NOTE: `initTopology` is used exclusively when `StreamTask` is requested to <<initializeTopology, initializeTopology>>.

=== [[initializeTopology]] Initializing Processor Topology -- `initializeTopology` Method

[source, java]
----
void initializeTopology()
----

NOTE: `initializeTopology` is part of <<kafka-streams-internals-Task.adoc#initializeTopology, Task Contract>> to initialize a <<kafka-streams-internals-ProcessorTopology.adoc#, ProcessorTopology>>.

`initializeTopology` <<initTopology, initTopology>>.

With <<kafka-streams-internals-AbstractTask.adoc#eosEnabled, exactly-once support enabled>>, `initializeTopology` requests the <<producer, Kafka Producer>> to start a new transaction (`Producer.beginTransaction`) and turns the <<transactionInFlight, transactionInFlight>> flag on.

`initializeTopology` then requests the <<kafka-streams-internals-AbstractTask.adoc#processorContext, InternalProcessorContext>> to <<kafka-streams-internals-InternalProcessorContext.adoc#initialize, initialize>>.

In the end, `initializeTopology` turns the <<kafka-streams-internals-AbstractTask.adoc#taskInitialized, taskInitialized>> flag on (`true`) and the <<idleStartTime, idleStartTime>> to `UNKNOWN`.

=== [[updateProcessorContext]] `updateProcessorContext` Internal Method

[source, java]
----
void updateProcessorContext(final StampedRecord record, final ProcessorNode currNode)
----

`updateProcessorContext`...FIXME

NOTE: `updateProcessorContext` is used when...FIXME

=== [[process]] Processing Single Record -- `process` Method

[source, java]
----
boolean process()
----

`process` requests <<partitionGroup, PartitionGroup>> for link:kafka-streams-internals-PartitionGroup.adoc#nextRecord[nextRecord] (with <<recordInfo, RecordInfo>>).

`process` prints out the following TRACE message to the logs:

```
Start processing one record [record]
```

`process` requests <<recordInfo, RecordInfo>> for the link:kafka-streams-internals-RecordInfo.adoc#node[source processor node].

`process` <<updateProcessorContext, updateProcessorContext>> (with the current record and the source processor node).

`process` requests the source processor node to link:kafka-streams-internals-ProcessorNode.adoc#process[process] the key and the value of the record.

`process` prints out the following TRACE message to the logs:

```
Completed processing one record [record]
```

`process` requests <<recordInfo, RecordInfo>> for the link:kafka-streams-internals-RecordInfo.adoc#partition[topic partition] and stores the partition and the record's link:kafka-streams-StampedRecord.adoc#offset[offset] in <<consumedOffsets, consumedOffsets>>.

`process` turns <<commitOffsetNeeded, commitOffsetNeeded>> flag on.

`process` requests the <<consumer, Kafka consumer>> to resume the partition if the size of the link:kafka-streams-internals-RecordInfo.adoc#queue[queue] of the <<recordInfo, RecordInfo>> is exactly <<maxBufferedSize, maxBufferedSize>>.

`process` always requests link:kafka-streams-internals-AbstractTask.adoc#processorContext[InternalProcessorContext] to link:kafka-streams-internals-InternalProcessorContext.adoc#setCurrentNode[setCurrentNode] as `null`.

In case of a `ProducerFencedException`, `process` reports a `TaskMigratedException`.

In case of a `KafkaException`, `process` reports a `StreamsException`.

In the end, `process` gives `true` when processing a single record was successful, and `false` when there were no records to process.

NOTE: `process` is used exclusively when `AssignedStreamsTasks` is requested to link:kafka-streams-AssignedStreamsTasks.adoc#process[process].

=== [[numBuffered]] `numBuffered` Method

[source, java]
----
int numBuffered()
----

`numBuffered`...FIXME

NOTE: `numBuffered` is used when...FIXME

=== [[closeSuspended]] `closeSuspended` Method

[source, java]
----
void closeSuspended(
  boolean clean,
  final boolean isZombie,
  RuntimeException firstException)
----

NOTE: `closeSuspended` is part of link:kafka-streams-internals-Task.adoc#closeSuspended[Task Contract] to...FIXME.

`closeSuspended`...FIXME

=== [[addRecords]] `addRecords` Method

[source, java]
----
int addRecords(
  final TopicPartition partition,
  final Iterable<ConsumerRecord<byte[], byte[]>> records)
----

`addRecords` requests <<partitionGroup, PartitionGroup>> to link:kafka-streams-internals-PartitionGroup.adoc#addRawRecords[add records to a RecordQueue for a Kafka partition].

You should see the following TRACE message in the logs:

```
Added records into the buffered queue of partition [partition], new queue size is [newQueueSize]"
```

`addRecords` requests the <<consumer, Kafka Consumer>> to pause the partition if the queue size of the partition exceeded <<maxBufferedSize, buffered.records.per.partition>> configuration property.

In the end, `addRecords` returns the number of records added.

NOTE: `addRecords` is used exclusively when `StreamThread` is requested to link:kafka-streams-StreamThread.adoc#addRecordsToTasks[add records to active stream tasks (and report skipped records)].

=== [[recordCollectorOffsets]] `recordCollectorOffsets` Method

[source, java]
----
Map<TopicPartition, Long> recordCollectorOffsets()
----

NOTE: `recordCollectorOffsets` is part of link:kafka-streams-internals-AbstractTask.adoc#recordCollectorOffsets[AbstractTask Contract] to...FIXME.

`recordCollectorOffsets`...FIXME

=== [[punctuate]] Executing Scheduled Periodic Action -- `punctuate` Method

[source, scala]
----
void punctuate(
  final ProcessorNode node,
  final long timestamp,
  final PunctuationType type,
  final Punctuator punctuator)
----

NOTE: `punctuate` is part of link:kafka-streams-ProcessorNodePunctuator.adoc#punctuate[ProcessorNodePunctuator Contract] to execute a scheduled periodic action.

`punctuate`...FIXME

=== [[maybePunctuateStreamTime]] `maybePunctuateStreamTime` Method

[source, java]
----
boolean maybePunctuateStreamTime()
----

`maybePunctuateStreamTime`...FIXME

NOTE: `maybePunctuateStreamTime` is used exclusively when `AssignedStreamsTasks` is requested to link:kafka-streams-AssignedStreamsTasks.adoc#punctuate[punctuate].

=== [[maybePunctuateSystemTime]] `maybePunctuateSystemTime` Method

[source, java]
----
boolean maybePunctuateSystemTime()
----

`maybePunctuateSystemTime`...FIXME

NOTE: `maybePunctuateSystemTime` is used exclusively when `AssignedStreamsTasks` is requested to link:kafka-streams-AssignedStreamsTasks.adoc#punctuate[punctuate].

=== [[schedule]] `schedule` Method

[source, scala]
----
// PUBLIC API
Cancellable schedule(
  final long interval,
  final PunctuationType type,
  final Punctuator punctuator)
// PACKAGE PROTECTED
Cancellable schedule(
  final long startTime,
  final long interval,
  final PunctuationType type,
  final Punctuator punctuator)
----

`schedule`...FIXME

NOTE: `schedule` is used exclusively when `ProcessorContextImpl` is requested to link:kafka-streams-internals-ProcessorContextImpl.adoc#schedule[schedule].

=== [[initializeStateStores]] Initializing State Stores -- `initializeStateStores` Method

[source, java]
----
boolean initializeStateStores()
----

NOTE: `initializeStateStores` is part of <<kafka-streams-internals-Task.adoc#initializeStateStores, Task Contract>> to initialize <<kafka-streams-StateStore.adoc#, state stores>>.

`initializeStateStores` prints out the following TRACE message to the logs:

```
Initializing state stores
```

`initializeStateStores` <<kafka-streams-internals-AbstractTask.adoc#registerStateStores, registerStateStores>>.

In the end, `initializeStateStores` returns `true` if the <<kafka-streams-internals-Task.adoc#changelogPartitions, task has any changelog partitions>>. Otherwise, `initializeStateStores` returns `false`.

=== [[commitOffsets]] `commitOffsets` Internal Method

[source, java]
----
void commitOffsets(final boolean startNewTransaction)
----

`commitOffsets`...FIXME

NOTE: `commitOffsets` is used exclusively when `StreamTask` is requested to <<commit, commit>>.

=== [[commit]] Committing Task -- `commit` Method

[source, java]
----
void commit()
----

NOTE: `commit` is part of <<kafka-streams-internals-Task.adoc#commit, Task Contract>> to commit the task.

`commit` simply <<commit-startNewTransaction, commits>> with the `startNewTransaction` flag on.

=== [[commit-startNewTransaction]] `commit` Internal Method

[source, java]
----
void commit(final boolean startNewTransaction)
----

`commit` prints out the following DEBUG message to the logs:

```
Committing
```

`commit` <<flushState, flushState>>.

(only when <<kafka-streams-internals-AbstractTask.adoc#eosEnabled, exactly-once support>> is off) `commit` requests the <<stateMgr, ProcessorStateManager>> to <<kafka-streams-ProcessorStateManager.adoc#checkpoint, checkpoint>> with the <<activeTaskCheckpointableOffsets, checkpointable offsets>>.

`commit` <<commitOffsets, commitOffsets>> with the input `startNewTransaction` flag.

`commit` turns the <<commitRequested, commitRequested>> internal flag off.

In the end, `commit` requests the <<taskMetrics, TaskMetrics>> for the <<taskCommitTimeSensor, taskCommitTimeSensor>> and records the duration (i.e. the time since `commit` was executed).

NOTE: `commit` is used when `StreamTask` is requested to <<commit, commit>> (that turns the input `startNewTransaction` flag on) and <<suspend, suspend>> (with the input `startNewTransaction` flag off).

=== [[activeTaskCheckpointableOffsets]] `activeTaskCheckpointableOffsets` Method

[source, java]
----
Map<TopicPartition, Long> activeTaskCheckpointableOffsets()
----

NOTE: `activeTaskCheckpointableOffsets` is part of the <<kafka-streams-internals-AbstractTask.adoc#activeTaskCheckpointableOffsets, AbstractTask Contract>> to return the checkpointable offsets.

`activeTaskCheckpointableOffsets`...FIXME

=== [[flushState]] Flushing State Stores And Producer (RecordCollector) -- `flushState` Method

[source, java]
----
void flushState()
----

NOTE: `flushState` is part of link:kafka-streams-internals-AbstractTask.adoc#flushState[AbstractTask Contract] to flush all <<kafka-streams-StateStore.adoc#, state stores>> registered with the task.

`flushState` prints out the following TRACE message to the logs:

```
Flushing state and producer
```

`flushState` <<kafka-streams-internals-AbstractTask.adoc#flushState, flushes state stores>>.

`flushState` requests the <<recordCollector, RecordCollector>> to <<kafka-streams-internals-RecordCollector.adoc#flush, flush the internal Kafka producer>>.

=== [[isProcessable]] `isProcessable` Method

[source, java]
----
boolean isProcessable(final long now)
----

`isProcessable`...FIXME

NOTE: `isProcessable` is used when...FIXME

=== [[resume]] `resume` Method

[source, java]
----
void resume()
----

NOTE: `resume` is part of the <<kafka-streams-internals-Task.adoc#resume, Task Contract>> to resume the task.

`resume`...FIXME

== [[maybeAbortTransactionAndCloseRecordCollector]] `maybeAbortTransactionAndCloseRecordCollector` Internal Method

[source, java]
----
void maybeAbortTransactionAndCloseRecordCollector(final boolean isZombie)
----

`maybeAbortTransactionAndCloseRecordCollector`...FIXME

NOTE: `maybeAbortTransactionAndCloseRecordCollector` is used when...FIXME

== [[initializeTransactions]] `initializeTransactions` Internal Method

[source, java]
----
void initializeTransactions()
----

`initializeTransactions`...FIXME

NOTE: `initializeTransactions` is used when...FIXME

== [[producerMetrics]] `producerMetrics` Method

[source, java]
----
Map<MetricName, Metric> producerMetrics()
----

`producerMetrics`...FIXME

NOTE: `producerMetrics` is used when...FIXME

=== [[internal-properties]] Internal Properties

.StreamTask's Internal Properties (e.g. Registries, Counters and Flags)
[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| partitionGroup
a| [[partitionGroup]] <<kafka-streams-internals-PartitionGroup.adoc#, PartitionGroup>> (with a <<kafka-streams-internals-RecordQueue.adoc#, RecordQueue>> for every assigned Kafka <<partitions, TopicPartition>>)

Used when `StreamTask` is requested for the following:

* <<addRecords, addRecords>>

* <<closeSuspended, closeSuspended>>

* <<closeTopology, closeTopology>>

* <<maybePunctuateStreamTime, maybePunctuateStreamTime>>

* <<numBuffered, numBuffered>>

* <<process, Process a single record>>

| consumedOffsets
| [[consumedOffsets]] Offsets by https://kafka.apache.org/20/javadoc/org/apache/kafka/common/TopicPartition.html[TopicPartitions] (`Map<TopicPartition, Long>`) that `StreamTask` has <<process, processed>> successfully

| idleStartTime
a| [[idleStartTime]]

| producer
a| [[producer]][[getProducer]] Kafka xref:https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#[Producer] (`Producer<byte[], byte[]>`)

Created when `StreamTask` is <<creating-instance, created>> and <<resume, resumed>> by requesting the <<producerSupplier, ProducerSupplier>> to <<kafka-streams-internals-ProducerSupplier.adoc#get, supply a Producer>>

Cleared (_nullified_) when `StreamTask` is requested to <<suspend, suspend>> and <<maybeAbortTransactionAndCloseRecordCollector, maybeAbortTransactionAndCloseRecordCollector>>

Used for the following:

* Requesting the <<recordCollector, RecordCollector>> to <<kafka-streams-internals-RecordCollector.adoc#init, initialize>> (when `StreamTask` is <<creating-instance, created>> and <<resume, resumed>>)

* <<initializeTopology, initializeTopology>>, <<initializeTransactions, initializeTransactions>>, <<maybeAbortTransactionAndCloseRecordCollector, maybeAbortTransactionAndCloseRecordCollector>>, and <<commit, commit>> for <<kafka-streams-exactly-once-support-eos.adoc#, exactly-once support>>

* <<producerMetrics, producerMetrics>>

| taskMetrics
a| [[taskMetrics]] <<kafka-streams-StreamTask-TaskMetrics.adoc#, TaskMetrics>> for the <<id, TaskId>> and the <<metrics, StreamsMetricsImpl>>

Used when `StreamTask` is requested for the following:

* <<isProcessable, isProcessable>> (to record an occurence of <<kafka-streams-StreamTask-TaskMetrics.adoc#taskEnforcedProcessSensor, taskEnforcedProcessSensor>> sensor)

* <<commit, commit>> (to record a value of <<kafka-streams-StreamTask-TaskMetrics.adoc#taskCommitTimeSensor, taskCommitTimeSensor>> sensor)

* <<closeSuspended, closeSuspended>> (to <<kafka-streams-StreamTask-TaskMetrics.adoc#removeAllSensors, remove all task sensors>>)

| transactionInFlight
a| [[transactionInFlight]] Controls whether...FIXME
|===
