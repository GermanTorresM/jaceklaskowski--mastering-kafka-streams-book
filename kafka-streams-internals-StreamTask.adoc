== [[StreamTask]] StreamTask

`StreamTask` is a concrete <<kafka-streams-internals-AbstractTask.adoc#, stream processor task>> that manages a <<partitionGroup, PartitionGroup>> (of the <<partitions, partitions assigned>>).

`StreamTask` is <<creating-instance, created>> exclusively when `TaskCreator` is requested to <<kafka-streams-internals-TaskCreator.adoc#createTask, create one>>.

When requested to <<initializeTopology, initialize a processor topology>> (as a <<kafka-streams-internals-Task.adoc#, task>>), `StreamTask`...FIXME

[[commitNeeded]]
[[commitRequested]]
[[needCommit]]
`StreamTask` may need a commit (after `ProcessorContextImpl` is requested to <<kafka-streams-internals-ProcessorContextImpl.adoc#commit, commit>>).

[[commitOffsetNeeded]]
`StreamTask` uses `commitOffsetNeeded` flag to...FIXME

[[maxBufferedSize]]
[[buffered.records.per.partition]]
`StreamTask` uses <<kafka-streams-properties.adoc#buffered.records.per.partition, buffered.records.per.partition>> configuration property (default: `1000`) to control whether to pause or resume a partition (when <<process, processing a single record>> or <<addRecords, bufferring new records>>, respectively).

[[maxTaskIdleMs]]
[[max.task.idle.ms]]
`StreamTask` uses <<kafka-streams-properties.adoc#max.task.idle.ms, max.task.idle.ms>> configuration property (default: `0L`) to...FIXME

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.kafka.streams.processor.internals.StreamTask` logger to see what happens inside.

Add the following line to `log4j.properties`:

```
log4j.logger.org.apache.kafka.streams.processor.internals.StreamTask=ALL
```

Refer to <<kafka-logging.adoc#log4j.properties, Application Logging Using log4j>>.
====

=== StreamTask and RecordCollector

`StreamTask` creates a new <<recordCollector, RecordCollector>> or is given one (when <<creating-instance, created>>).

The <<recordCollector, RecordCollector>> is requested to <<kafka-streams-internals-RecordCollector.adoc#init, initialize>> (with a <<producer, Kafka Producer>>) when `StreamTask` is <<creating-instance, created>> and <<resume, resumed>>.

`StreamTask` uses the <<recordCollector, RecordCollector>> for the following:

* Creating an <<kafka-streams-internals-AbstractTask.adoc#processorContext, InternalProcessorContext>> (when <<creating-instance, created>>)

* <<kafka-streams-internals-RecordCollector.adoc#offsets, Getting offsets>> when <<activeTaskCheckpointableOffsets, activeTaskCheckpointableOffsets>>

* <<kafka-streams-internals-RecordCollector.adoc#flush, Flushing the internal Kafka producer>> when <<flushState, flushState>>

* <<suspend, suspend>> and <<maybeAbortTransactionAndCloseRecordCollector, maybeAbortTransactionAndCloseRecordCollector>>

The <<recordCollector, RecordCollector>> is requested to <<kafka-streams-internals-RecordCollector.adoc#close, close>> when `StreamTask` is requested to <<suspend, suspend>> and <<maybeAbortTransactionAndCloseRecordCollector, maybeAbortTransactionAndCloseRecordCollector>>.

=== [[closeTopology]] `closeTopology` Internal Method

[source, java]
----
void closeTopology()
----

`closeTopology`...FIXME

NOTE: `closeTopology` is used exclusively when `StreamTask` is requested to <<suspend, suspend>>.

=== [[suspend]] `suspend` Method

[source, java]
----
void suspend()  // <1>

// PRIVATE API
suspend(boolean clean, boolean isZombie)
----
<1> Uses the private `suspend` with `clean` enabled and `isZombie` disabled

NOTE: `suspend` is part of <<kafka-streams-internals-Task.adoc#suspend, Task Contract>> to...FIXME

`suspend`...FIXME

NOTE: The private `suspend` is used when `StreamTask` is requested to <<close, close>>.

=== [[close]] `close` Method

[source, java]
----
void close(
  final boolean clean,
  final boolean isZombie)
----

NOTE: `close` is part of link:kafka-streams-internals-Task.adoc#close[Task Contract] to...FIXME.

`close`...FIXME

=== [[creating-instance]] Creating StreamTask Instance

`StreamTask` takes the following to be created:

* [[id]] <<kafka-streams-TaskId.adoc#, TaskId>>
* [[partitions]] Topic partitions (Kafka `TopicPartition`)
* [[topology]] <<kafka-streams-internals-ProcessorTopology.adoc#, ProcessorTopology>>
* [[consumer]] Kafka https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[Consumer] (`Consumer<byte[], byte[]>`)
* [[changelogReader]] <<kafka-streams-ChangelogReader.adoc#, ChangelogReader>>
* [[config]] <<kafka-streams-StreamsConfig.adoc#, StreamsConfig>>
* [[metrics]] <<kafka-streams-internals-StreamsMetricsImpl.adoc#, StreamsMetricsImpl>>
* [[stateDirectory]] <<kafka-streams-internals-StateDirectory.adoc#, StateDirectory>>
* [[cache]] <<kafka-streams-internals-ThreadCache.adoc#, ThreadCache>>
* [[time]] `Time`
* [[producerSupplier]] <<kafka-streams-internals-ProducerSupplier.adoc#, ProducerSupplier>>
* [[recordCollector]] <<kafka-streams-internals-RecordCollector.adoc#, RecordCollector>>
* [[closeSensor]] `closeSensor` Kafka `Sensor`

`StreamTask` initializes the <<internal-properties, internal properties>>.

=== [[initTopology]] `initTopology` Internal Method

[source, java]
----
void initTopology()
----

`initTopology`...FIXME

NOTE: `initTopology` is used exclusively when `StreamTask` is requested to <<initializeTopology, initializeTopology>>.

=== [[initializeTopology]] Initializing Processor Topology -- `initializeTopology` Method

[source, java]
----
void initializeTopology()
----

NOTE: `initializeTopology` is part of <<kafka-streams-internals-Task.adoc#initializeTopology, Task Contract>> to initialize a <<kafka-streams-internals-ProcessorTopology.adoc#, ProcessorTopology>>.

`initializeTopology` <<initTopology, initTopology>>.

With <<kafka-streams-internals-AbstractTask.adoc#eosEnabled, exactly-once support enabled>>, `initializeTopology` requests the <<producer, Kafka Producer>> to start a new transaction (using link:++https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#beginTransaction--++[Producer.beginTransaction]) and turns the <<transactionInFlight, transactionInFlight>> flag on.

`initializeTopology` then requests the <<kafka-streams-internals-AbstractTask.adoc#processorContext, InternalProcessorContext>> to <<kafka-streams-internals-InternalProcessorContext.adoc#initialize, initialize>>.

In the end, `initializeTopology` turns the <<kafka-streams-internals-AbstractTask.adoc#taskInitialized, taskInitialized>> flag on (`true`) and the <<idleStartTime, idleStartTime>> to `UNKNOWN`.

=== [[updateProcessorContext]] `updateProcessorContext` Internal Method

[source, java]
----
void updateProcessorContext(final StampedRecord record, final ProcessorNode currNode)
----

`updateProcessorContext`...FIXME

NOTE: `updateProcessorContext` is used when...FIXME

=== [[process]] Processing Single Record -- `process` Method

[source, java]
----
boolean process()
----

`process` requests the <<partitionGroup, PartitionGroup>> for <<kafka-streams-internals-PartitionGroup.adoc#nextRecord, nextRecord>> (with <<recordInfo, RecordInfo>>).

`process` prints out the following TRACE message to the logs:

```
Start processing one record [record]
```

`process` requests the <<recordInfo, RecordInfo>> for the <<kafka-streams-internals-RecordInfo.adoc#node, source processor node>>.

`process` <<updateProcessorContext, updateProcessorContext>> (with the current record and the source processor node).

`process` requests the source processor node to <<kafka-streams-internals-ProcessorNode.adoc#process, process>> the key and the value of the record.

`process` prints out the following TRACE message to the logs:

```
Completed processing one record [record]
```

`process` requests the <<recordInfo, RecordInfo>> for the link:kafka-streams-internals-RecordInfo.adoc#partition, partition>> and stores it and the record's <<kafka-streams-internals-StampedRecord.adoc#offset, offset>> in the <<consumedOffsets, consumedOffsets>> internal registry.

`process` turns the <<commitOffsetNeeded, commitOffsetNeeded>> flag on.

`process` requests the <<consumer, Kafka consumer>> to resume the partition if the size of the <<kafka-streams-internals-RecordInfo.adoc#queue, queue>> of the <<recordInfo, RecordInfo>> is exactly <<maxBufferedSize, buffered.records.per.partition>> configuration property (default: `1000`).

`process` always requests <<kafka-streams-internals-AbstractTask.adoc#processorContext, InternalProcessorContext>> to <<kafka-streams-internals-InternalProcessorContext.adoc#setCurrentNode, setCurrentNode>> as `null`.

In case of a `ProducerFencedException`, `process` throws a `TaskMigratedException`.

In case of a `KafkaException`, `process` throws a `StreamsException`.

In the end, `process` returns `true` when the single record was processed successfully, and `false` when there were no records to process.

NOTE: `process` is used exclusively when `AssignedStreamsTasks` is requested to <<kafka-streams-AssignedStreamsTasks.adoc#process, request the running stream tasks to process records (one record per task)>>.

=== [[numBuffered]] `numBuffered` Method

[source, java]
----
int numBuffered()
----

`numBuffered`...FIXME

NOTE: `numBuffered` is used when...FIXME

=== [[closeSuspended]] `closeSuspended` Method

[source, java]
----
void closeSuspended(
  boolean clean,
  final boolean isZombie,
  RuntimeException firstException)
----

NOTE: `closeSuspended` is part of link:kafka-streams-internals-Task.adoc#closeSuspended[Task Contract] to...FIXME.

`closeSuspended`...FIXME

=== [[addRecords]] Buffering New Records (From TopicPartition) -- `addRecords` Method

[source, java]
----
void addRecords(
  final TopicPartition partition,
  final Iterable<ConsumerRecord<byte[], byte[]>> records)
----

`addRecords` simply requests the <<partitionGroup, PartitionGroup>> to <<kafka-streams-internals-PartitionGroup.adoc#addRawRecords, add the new records to the RecordQueue for the specified partition>>.

.StreamTask and Buffering New Records
image::images/kafka-streams-StreamTask-addRecords.png[align="center"]

`addRecords` prints out the following TRACE message to the logs:

```
Added records into the buffered queue of partition [partition], new queue size is [newQueueSize]
```

When the size of the buffered record queue exceeds <<buffered.records.per.partition, buffered.records.per.partition>> configuration property, `addRecords` requests the <<consumer, Kafka Consumer>> to pause the partition.

NOTE: `addRecords` uses link:++https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#pause-java.util.Collection-++[Consumer.pause] method to _"pause the partition"_, i.e. to suspend fetching from the requested partitions. Future calls to link:++https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#poll-java.time.Duration-++[KafkaConsumer.poll] will not return any records from these partitions until they have been resumed using link:++https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#resume-java.util.Collection-++[KafkaConsumer.resume].

[NOTE]
====
`addRecords` is used when:

* `StreamThread` is requested to <<kafka-streams-internals-StreamThread.adoc#addRecordsToTasks, add records to active stream tasks (and report skipped records)>>

* `TopologyTestDriver` is requested to <<kafka-streams-TopologyTestDriver.adoc#pipeInput, pipeInput>>
====

=== [[recordCollectorOffsets]] `recordCollectorOffsets` Method

[source, java]
----
Map<TopicPartition, Long> recordCollectorOffsets()
----

NOTE: `recordCollectorOffsets` is part of link:kafka-streams-internals-AbstractTask.adoc#recordCollectorOffsets[AbstractTask Contract] to...FIXME.

`recordCollectorOffsets`...FIXME

=== [[punctuate]] Executing Scheduled Periodic Action -- `punctuate` Method

[source, scala]
----
void punctuate(
  final ProcessorNode node,
  final long timestamp,
  final PunctuationType type,
  final Punctuator punctuator)
----

NOTE: `punctuate` is part of link:kafka-streams-ProcessorNodePunctuator.adoc#punctuate[ProcessorNodePunctuator Contract] to execute a scheduled periodic action.

`punctuate`...FIXME

=== [[maybePunctuateStreamTime]] `maybePunctuateStreamTime` Method

[source, java]
----
boolean maybePunctuateStreamTime()
----

`maybePunctuateStreamTime`...FIXME

NOTE: `maybePunctuateStreamTime` is used exclusively when `AssignedStreamsTasks` is requested to link:kafka-streams-AssignedStreamsTasks.adoc#punctuate[punctuate].

=== [[maybePunctuateSystemTime]] `maybePunctuateSystemTime` Method

[source, java]
----
boolean maybePunctuateSystemTime()
----

`maybePunctuateSystemTime`...FIXME

NOTE: `maybePunctuateSystemTime` is used exclusively when `AssignedStreamsTasks` is requested to link:kafka-streams-AssignedStreamsTasks.adoc#punctuate[punctuate].

=== [[schedule]] `schedule` Method

[source, scala]
----
// PUBLIC API
Cancellable schedule(
  final long interval,
  final PunctuationType type,
  final Punctuator punctuator)
// PACKAGE PROTECTED
Cancellable schedule(
  final long startTime,
  final long interval,
  final PunctuationType type,
  final Punctuator punctuator)
----

`schedule`...FIXME

NOTE: `schedule` is used exclusively when `ProcessorContextImpl` is requested to link:kafka-streams-internals-ProcessorContextImpl.adoc#schedule[schedule].

=== [[initializeStateStores]] Initializing State Stores -- `initializeStateStores` Method

[source, java]
----
boolean initializeStateStores()
----

NOTE: `initializeStateStores` is part of <<kafka-streams-internals-Task.adoc#initializeStateStores, Task Contract>> to initialize <<kafka-streams-StateStore.adoc#, state stores>>.

`initializeStateStores` prints out the following TRACE message to the logs:

```
Initializing state stores
```

`initializeStateStores` <<kafka-streams-internals-AbstractTask.adoc#registerStateStores, registerStateStores>>.

In the end, `initializeStateStores` returns `true` if the <<kafka-streams-internals-Task.adoc#changelogPartitions, task has any changelog partitions>>. Otherwise, `initializeStateStores` returns `false`.

=== [[commitOffsets]] `commitOffsets` Internal Method

[source, java]
----
void commitOffsets(final boolean startNewTransaction)
----

`commitOffsets`...FIXME

NOTE: `commitOffsets` is used exclusively when `StreamTask` is requested to <<commit, commit>>.

=== [[commit]] Committing Task -- `commit` Method

[source, java]
----
void commit()
----

NOTE: `commit` is part of <<kafka-streams-internals-Task.adoc#commit, Task Contract>> to commit the task.

`commit` simply <<commit-startNewTransaction, commits>> with the `startNewTransaction` flag on.

=== [[commit-startNewTransaction]] `commit` Internal Method

[source, java]
----
void commit(final boolean startNewTransaction)
----

`commit` prints out the following DEBUG message to the logs:

```
Committing
```

`commit` <<flushState, flushState>>.

(only when <<kafka-streams-internals-AbstractTask.adoc#eosEnabled, exactly-once support>> is off) `commit` requests the <<stateMgr, ProcessorStateManager>> to <<kafka-streams-ProcessorStateManager.adoc#checkpoint, checkpoint>> with the <<activeTaskCheckpointableOffsets, checkpointable offsets>>.

`commit` <<commitOffsets, commitOffsets>> with the input `startNewTransaction` flag.

`commit` turns the <<commitRequested, commitRequested>> internal flag off.

In the end, `commit` requests the <<taskMetrics, TaskMetrics>> for the <<taskCommitTimeSensor, taskCommitTimeSensor>> and records the duration (i.e. the time since `commit` was executed).

NOTE: `commit` is used when `StreamTask` is requested to <<commit, commit>> (that turns the input `startNewTransaction` flag on) and <<suspend, suspend>> (with the input `startNewTransaction` flag off).

=== [[activeTaskCheckpointableOffsets]] `activeTaskCheckpointableOffsets` Method

[source, java]
----
Map<TopicPartition, Long> activeTaskCheckpointableOffsets()
----

NOTE: `activeTaskCheckpointableOffsets` is part of the <<kafka-streams-internals-AbstractTask.adoc#activeTaskCheckpointableOffsets, AbstractTask Contract>> to return the checkpointable offsets.

`activeTaskCheckpointableOffsets`...FIXME

=== [[flushState]] Flushing State Stores And Producer (RecordCollector) -- `flushState` Method

[source, java]
----
void flushState()
----

NOTE: `flushState` is part of link:kafka-streams-internals-AbstractTask.adoc#flushState[AbstractTask Contract] to flush all <<kafka-streams-StateStore.adoc#, state stores>> registered with the task.

`flushState` prints out the following TRACE message to the logs:

```
Flushing state and producer
```

`flushState` <<kafka-streams-internals-AbstractTask.adoc#flushState, flushes state stores>>.

`flushState` requests the <<recordCollector, RecordCollector>> to <<kafka-streams-internals-RecordCollector.adoc#flush, flush the internal Kafka producer>>.

=== [[isProcessable]] `isProcessable` Method

[source, java]
----
boolean isProcessable(final long now)
----

`isProcessable` returns `true` when one of the following is met:

* <<kafka-streams-internals-PartitionGroup.adoc#allPartitionsBuffered, allPartitionsBuffered>> of the <<partitionGroup, PartitionGroup>> (default: `false`)

* The task is enforced to be processable, i.e. the time between `now` and the <<idleStartTime, idleStartTime>> is at least or larger than the <<maxTaskIdleMs, max.task.idle.ms>> configuration property (default: `0L`)

Otherwise, `isProcessable` returns `false`.

NOTE: (FIXME) `isProcessable` does some minor accounting.

NOTE: `isProcessable` is used exclusively when `AssignedStreamsTasks` is requested to <<kafka-streams-AssignedStreamsTasks.adoc#process, request the running stream tasks to process records (one record per task)>>.

=== [[resume]] Resuming Task -- `resume` Method

[source, java]
----
void resume()
----

NOTE: `resume` is part of the <<kafka-streams-internals-Task.adoc#resume, Task Contract>> to resume the task.

`resume` prints out the following DEBUG message to the logs:

```
Resuming
```

`resume` then does further processing only when <<kafka-streams-internals-AbstractTask.adoc#eosEnabled, Exactly-Once Support>> is enabled.

`resume`...FIXME

== [[maybeAbortTransactionAndCloseRecordCollector]] `maybeAbortTransactionAndCloseRecordCollector` Internal Method

[source, java]
----
void maybeAbortTransactionAndCloseRecordCollector(final boolean isZombie)
----

`maybeAbortTransactionAndCloseRecordCollector`...FIXME

NOTE: `maybeAbortTransactionAndCloseRecordCollector` is used when...FIXME

== [[initializeTransactions]] `initializeTransactions` Internal Method

[source, java]
----
void initializeTransactions()
----

`initializeTransactions`...FIXME

NOTE: `initializeTransactions` is used when...FIXME

== [[producerMetrics]] `producerMetrics` Method

[source, java]
----
Map<MetricName, Metric> producerMetrics()
----

`producerMetrics`...FIXME

NOTE: `producerMetrics` is used when...FIXME

=== [[internal-properties]] Internal Properties

.StreamTask's Internal Properties (e.g. Registries, Counters and Flags)
[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| partitionGroup
a| [[partitionGroup]] <<kafka-streams-internals-PartitionGroup.adoc#, PartitionGroup>> with <<kafka-streams-internals-RecordQueue.adoc#, RecordQueues>> (for every <<partitions, Kafka partition assigned>>)

Used when `StreamTask` is requested for the following:

* <<isProcessable, isProcessable>>

* <<process, Process a single record>>

* <<closeTopology, closeTopology>>

* <<closeSuspended, closeSuspended>>

* <<addRecords, addRecords>>

* <<numBuffered, numBuffered>>

* <<maybePunctuateStreamTime, maybePunctuateStreamTime>>

| consumedOffsets
| [[consumedOffsets]] Offsets by https://kafka.apache.org/22/javadoc/org/apache/kafka/common/TopicPartition.html[TopicPartitions] (`Map<TopicPartition, Long>`) that `StreamTask` has <<process, processed>> successfully

| idleStartTime
a| [[idleStartTime]]

| producer
a| [[producer]][[getProducer]] Kafka xref:https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#[Producer] (`Producer<byte[], byte[]>`)

Created when `StreamTask` is <<creating-instance, created>> and <<resume, resumed>> by requesting the <<producerSupplier, ProducerSupplier>> to <<kafka-streams-internals-ProducerSupplier.adoc#get, supply a Producer>>

Cleared (_nullified_) when `StreamTask` is requested to <<suspend, suspend>> and <<maybeAbortTransactionAndCloseRecordCollector, maybeAbortTransactionAndCloseRecordCollector>>

Used for the following:

* Requesting the <<recordCollector, RecordCollector>> to <<kafka-streams-internals-RecordCollector.adoc#init, initialize>> (when `StreamTask` is <<creating-instance, created>> and <<resume, resumed>>)

* <<initializeTopology, initializeTopology>>, <<initializeTransactions, initializeTransactions>>, <<maybeAbortTransactionAndCloseRecordCollector, maybeAbortTransactionAndCloseRecordCollector>>, and <<commit, commit>> for <<kafka-streams-exactly-once-support-eos.adoc#, exactly-once support>>

* <<producerMetrics, producerMetrics>>

| processorContext
a| [[processorContext]] <<kafka-streams-internals-ProcessorContextImpl.adoc#, ProcessorContextImpl>>

| recordInfo
a| [[recordInfo]] <<kafka-streams-internals-RecordInfo.adoc#, RecordInfo>>

| streamTimePunctuationQueue
a| [[streamTimePunctuationQueue]] <<kafka-streams-PunctuationQueue.adoc#, PunctuationQueue>>

| systemTimePunctuationQueue
a| [[systemTimePunctuationQueue]] <<kafka-streams-PunctuationQueue.adoc#, PunctuationQueue>>

| taskMetrics
a| [[taskMetrics]] <<kafka-streams-StreamTask-TaskMetrics.adoc#, TaskMetrics>> for the <<id, TaskId>> and the <<metrics, StreamsMetricsImpl>>

Used when `StreamTask` is requested for the following:

* <<isProcessable, isProcessable>> (to record an occurence of <<kafka-streams-StreamTask-TaskMetrics.adoc#taskEnforcedProcessSensor, taskEnforcedProcessSensor>> sensor)

* <<commit, commit>> (to record a value of <<kafka-streams-StreamTask-TaskMetrics.adoc#taskCommitTimeSensor, taskCommitTimeSensor>> sensor)

* <<closeSuspended, closeSuspended>> (to <<kafka-streams-StreamTask-TaskMetrics.adoc#removeAllSensors, remove all task sensors>>)

| transactionInFlight
a| [[transactionInFlight]] Controls whether...FIXME
|===
