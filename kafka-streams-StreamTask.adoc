== [[StreamTask]] StreamTask

`StreamTask` is a concrete link:kafka-streams-AbstractTask.adoc[stream processor task] that manages a <<partitions, collection of Kafka TopicPartitions>> as a <<partitionGroup, PartitionGroup>>.

`StreamTask` is <<creating-instance, created>> exclusively when `TaskCreator` is requested to <<kafka-streams-TaskCreator.adoc#createTask, create one>>.

[[commitNeeded]]
[[commitRequested]]
[[needCommit]]
`StreamTask` may need a commit (after `ProcessorContextImpl` is requested to <<kafka-streams-ProcessorContextImpl.adoc#commit, commit>>).

[[commitOffsetNeeded]]
`StreamTask` uses `commitOffsetNeeded` flag to...FIXME

[[maxBufferedSize]]
`StreamTask` uses link:kafka-streams-StreamsConfig.adoc#buffered.records.per.partition[buffered.records.per.partition] configuration property to control when to resume a partition (when <<process, processing a single record>>).

[[internal-registries]]
.StreamTask's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,2",options="header",width="100%"]
|===
| Name
| Description

| partitionGroup
a| [[partitionGroup]] <<kafka-streams-PartitionGroup.adoc#, PartitionGroup>> (with a <<kafka-streams-RecordQueue.adoc#, RecordQueue>> for every assigned Kafka <<partitions, TopicPartition>>)

Used when `StreamTask` is requested for the following:

* <<addRecords, addRecords>>

* <<closeSuspended, closeSuspended>>

* <<closeTopology, closeTopology>>

* <<maybePunctuateStreamTime, maybePunctuateStreamTime>>

* <<numBuffered, numBuffered>>

* <<process, Process a single record>>

| consumedOffsets
| [[consumedOffsets]] Offsets by https://kafka.apache.org/20/javadoc/org/apache/kafka/common/TopicPartition.html[TopicPartitions] (`Map<TopicPartition, Long>`) that `StreamTask` has <<process, processed>> successfully

| taskMetrics
| [[taskMetrics]] <<kafka-streams-StreamTask-TaskMetrics.adoc#, TaskMetrics>>
|===

[[logging]]
[TIP]
====
Enable `TRACE` logging level for `org.apache.kafka.streams.processor.internals.StreamTask` logger to see what happens inside.

Add the following line to `log4j.properties`:

```
log4j.logger.org.apache.kafka.streams.processor.internals.StreamTask=TRACE
```

Refer to link:kafka-logging.adoc#log4j.properties[Application Logging Using log4j].
====

=== [[closeTopology]] `closeTopology` Internal Method

[source, java]
----
void closeTopology()
----

`closeTopology`...FIXME

NOTE: `closeTopology` is used exclusively when `StreamTask` is requested to <<suspend, suspend>>.

=== [[suspend]] `suspend` Method

[source, java]
----
void suspend()  // <1>

// PRIVATE API
void suspend(final boolean clean)
----
<1> Uses `clean` flag enabled, i.e. `true`

NOTE: `suspend` is part of <<kafka-streams-Task.adoc#suspend, Task Contract>> to...FIXME

`suspend`...FIXME

NOTE: The private `suspend` is used exclusively when `StreamTask` is requested to <<close, close>>.

=== [[close]] `close` Method

[source, java]
----
void close(
  final boolean clean,
  final boolean isZombie)
----

NOTE: `close` is part of link:kafka-streams-Task.adoc#close[Task Contract] to...FIXME.

`close`...FIXME

=== [[creating-instance]] Creating StreamTask Instance

`StreamTask` takes the following when created:

* [[id]] `TaskId`
* [[partitions]] Topic partitions (as Kafka's `TopicPartition`)
* [[topology]] link:kafka-streams-ProcessorTopology.adoc[ProcessorTopology]
* [[consumer]] Kafka https://kafka.apache.org/20/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[Consumer] (`Consumer<byte[], byte[]>`)
* [[changelogReader]] link:kafka-streams-ChangelogReader.adoc[ChangelogReader]
* [[config]] link:kafka-streams-StreamsConfig.adoc[StreamsConfig]
* [[metrics]] link:kafka-streams-StreamsMetrics.adoc[StreamsMetrics]
* [[stateDirectory]] link:kafka-streams-StateDirectory.adoc[StateDirectory]
* [[cache]] `ThreadCache`
* [[time]] `Time`
* [[producer]] Kafka `Producer` (of keys and values as array of bytes)

`StreamTask` initializes the <<internal-registries, internal registries and counters>>.

=== [[initTopology]] `initTopology` Internal Method

[source, java]
----
void initTopology()
----

`initTopology`...FIXME

NOTE: `initTopology` is used when...FIXME

=== [[initializeTopology]] `initializeTopology` Method

[source, java]
----
void initializeTopology()
----

NOTE: `initializeTopology` is part of link:kafka-streams-Task.adoc#initializeTopology[Task Contract] to...FIXME.

`initializeTopology` <<initTopology, initTopology>>. It then requests link:kafka-streams-AbstractTask.adoc#processorContext[InternalProcessorContext] to link:kafka-streams-InternalProcessorContext.adoc#initialized[initialized] and in the end sets link:kafka-streams-AbstractTask.adoc#taskInitialized[taskInitialized] to `true`.

=== [[updateProcessorContext]] `updateProcessorContext` Internal Method

[source, java]
----
void updateProcessorContext(final StampedRecord record, final ProcessorNode currNode)
----

`updateProcessorContext`...FIXME

NOTE: `updateProcessorContext` is used when...FIXME

=== [[process]] Processing Single Record -- `process` Method

[source, java]
----
boolean process()
----

`process` requests <<partitionGroup, PartitionGroup>> for link:kafka-streams-PartitionGroup.adoc#nextRecord[nextRecord] (with <<recordInfo, RecordInfo>>).

`process` prints out the following TRACE message to the logs:

```
Start processing one record [record]
```

`process` requests <<recordInfo, RecordInfo>> for the link:kafka-streams-RecordInfo.adoc#node[source processor node].

`process` <<updateProcessorContext, updateProcessorContext>> (with the current record and the source processor node).

`process` requests the source processor node to link:kafka-streams-ProcessorNode.adoc#process[process] the key and the value of the record.

`process` prints out the following TRACE message to the logs:

```
Completed processing one record [record]
```

`process` requests <<recordInfo, RecordInfo>> for the link:kafka-streams-RecordInfo.adoc#partition[topic partition] and stores the partition and the record's link:kafka-streams-StampedRecord.adoc#offset[offset] in <<consumedOffsets, consumedOffsets>>.

`process` turns <<commitOffsetNeeded, commitOffsetNeeded>> flag on.

`process` requests the <<consumer, Kafka consumer>> to resume the partition if the size of the link:kafka-streams-RecordInfo.adoc#queue[queue] of the <<recordInfo, RecordInfo>> is exactly <<maxBufferedSize, maxBufferedSize>>.

`process` always requests link:kafka-streams-AbstractTask.adoc#processorContext[InternalProcessorContext] to link:kafka-streams-InternalProcessorContext.adoc#setCurrentNode[setCurrentNode] as `null`.

In case of a `ProducerFencedException`, `process` reports a `TaskMigratedException`.

In case of a `KafkaException`, `process` reports a `StreamsException`.

In the end, `process` gives `true` when processing a single record was successful, and `false` when there were no records to process.

NOTE: `process` is used exclusively when `AssignedStreamsTasks` is requested to link:kafka-streams-AssignedStreamsTasks.adoc#process[process].

=== [[numBuffered]] `numBuffered` Method

[source, java]
----
int numBuffered()
----

`numBuffered`...FIXME

NOTE: `numBuffered` is used when...FIXME

=== [[closeSuspended]] `closeSuspended` Method

[source, java]
----
void closeSuspended(
  boolean clean,
  final boolean isZombie,
  RuntimeException firstException)
----

NOTE: `closeSuspended` is part of link:kafka-streams-Task.adoc#closeSuspended[Task Contract] to...FIXME.

`closeSuspended`...FIXME

=== [[addRecords]] `addRecords` Method

[source, java]
----
int addRecords(
  final TopicPartition partition,
  final Iterable<ConsumerRecord<byte[], byte[]>> records)
----

`addRecords` requests <<partitionGroup, PartitionGroup>> to link:kafka-streams-PartitionGroup.adoc#addRawRecords[add records to a RecordQueue for a Kafka partition].

You should see the following TRACE message in the logs:

```
Added records into the buffered queue of partition [partition], new queue size is [newQueueSize]"
```

`addRecords` requests the <<consumer, Kafka Consumer>> to pause the partition if the queue size of the partition exceeded <<maxBufferedSize, buffered.records.per.partition>> configuration property.

In the end, `addRecords` returns the number of records added.

NOTE: `addRecords` is used exclusively when `StreamThread` is requested to link:kafka-streams-StreamThread.adoc#addRecordsToTasks[add records to active stream tasks (and report skipped records)].

=== [[recordCollectorOffsets]] `recordCollectorOffsets` Method

[source, java]
----
Map<TopicPartition, Long> recordCollectorOffsets()
----

NOTE: `recordCollectorOffsets` is part of link:kafka-streams-AbstractTask.adoc#recordCollectorOffsets[AbstractTask Contract] to...FIXME.

`recordCollectorOffsets`...FIXME

=== [[punctuate]] Executing Scheduled Periodic Action -- `punctuate` Method

[source, scala]
----
void punctuate(
  final ProcessorNode node,
  final long timestamp,
  final PunctuationType type,
  final Punctuator punctuator)
----

NOTE: `punctuate` is part of link:kafka-streams-ProcessorNodePunctuator.adoc#punctuate[ProcessorNodePunctuator Contract] to execute a scheduled periodic action.

`punctuate`...FIXME

=== [[maybePunctuateStreamTime]] `maybePunctuateStreamTime` Method

[source, java]
----
boolean maybePunctuateStreamTime()
----

`maybePunctuateStreamTime`...FIXME

NOTE: `maybePunctuateStreamTime` is used exclusively when `AssignedStreamsTasks` is requested to link:kafka-streams-AssignedStreamsTasks.adoc#punctuate[punctuate].

=== [[maybePunctuateSystemTime]] `maybePunctuateSystemTime` Method

[source, java]
----
boolean maybePunctuateSystemTime()
----

`maybePunctuateSystemTime`...FIXME

NOTE: `maybePunctuateSystemTime` is used exclusively when `AssignedStreamsTasks` is requested to link:kafka-streams-AssignedStreamsTasks.adoc#punctuate[punctuate].

=== [[schedule]] `schedule` Method

[source, scala]
----
// PUBLIC API
Cancellable schedule(
  final long interval,
  final PunctuationType type,
  final Punctuator punctuator)
// PACKAGE PROTECTED
Cancellable schedule(
  final long startTime,
  final long interval,
  final PunctuationType type,
  final Punctuator punctuator)
----

`schedule`...FIXME

NOTE: `schedule` is used exclusively when `ProcessorContextImpl` is requested to link:kafka-streams-ProcessorContextImpl.adoc#schedule[schedule].

=== [[initializeStateStores]] Initializing State Stores -- `initializeStateStores` Method

[source, java]
----
boolean initializeStateStores()
----

NOTE: `initializeStateStores` is part of <<kafka-streams-Task.adoc#initializeStateStores, Task Contract>> to initialize <<kafka-streams-StateStore.adoc#, state stores>>.

`initializeStateStores` prints out the following TRACE message to the logs:

```
Initializing state stores
```

`initializeStateStores` <<kafka-streams-AbstractTask.adoc#registerStateStores, registerStateStores>>.

In the end, `initializeStateStores` returns `true` if the <<kafka-streams-Task.adoc#changelogPartitions, task has any changelog partitions>>. Otherwise, `initializeStateStores` returns `false`.

=== [[commitOffsets]] `commitOffsets` Internal Method

[source, java]
----
void commitOffsets(final boolean startNewTransaction)
----

`commitOffsets`...FIXME

NOTE: `commitOffsets` is used exclusively when `StreamTask` is requested to <<commit, commit>>.

=== [[commit]] Committing Task -- `commit` Method

[source, java]
----
void commit()
----

NOTE: `commit` is part of <<kafka-streams-Task.adoc#commit, Task Contract>> to commit the task.

`commit` simply <<commit-startNewTransaction, commits>> with the `startNewTransaction` flag on.

=== [[commit-startNewTransaction]] `commit` Internal Method

[source, java]
----
void commit(final boolean startNewTransaction)
----

`commit` prints out the following DEBUG message to the logs:

```
Committing
```

`commit` <<flushState, flushState>>.

(only when <<kafka-streams-AbstractTask.adoc#eosEnabled, exactly-once support>> is disabled) `commit` requests the <<stateMgr, ProcessorStateManager>> to <<kafka-streams-ProcessorStateManager.adoc#checkpoint, checkpoint>> with the <<activeTaskCheckpointableOffsets, checkpointable offsets>>.

`commit` <<commitOffsets, commitOffsets>> with the input `startNewTransaction` flag.

`commit` turns the <<commitRequested, commitRequested>> internal flag off.

In the end, `commit` requests the <<taskMetrics, TaskMetrics>> for the <<taskCommitTimeSensor, taskCommitTimeSensor>> and records the duration (i.e. the time since `commit` was executed).

NOTE: `commit` is used when `StreamTask` is requested to <<commit, commit>> (that turns the input `startNewTransaction` flag on) and <<suspend, suspend>> (with the input `startNewTransaction` flag off).

=== [[activeTaskCheckpointableOffsets]] `activeTaskCheckpointableOffsets` Method

[source, java]
----
Map<TopicPartition, Long> activeTaskCheckpointableOffsets()
----

NOTE: `activeTaskCheckpointableOffsets` is part of the <<kafka-streams-AbstractTask.adoc#activeTaskCheckpointableOffsets, AbstractTask Contract>> to return the checkpointable offsets.

`activeTaskCheckpointableOffsets`...FIXME

=== [[flushState]] Flushing State Stores And Producer (RecordCollector) -- `flushState` Method

[source, java]
----
void flushState()
----

NOTE: `flushState` is part of link:kafka-streams-AbstractTask.adoc#flushState[AbstractTask Contract] to flush all <<kafka-streams-StateStore.adoc#, state stores>> registered with the task.

`flushState` prints out the following TRACE message to the logs:

```
Flushing state and producer
```

`flushState` <<kafka-streams-AbstractTask.adoc#flushState, flushes state stores>>.

`flushState` requests the <<recordCollector, RecordCollector>> to <<kafka-streams-RecordCollector.adoc#flush, flush>>.
